#!/usr/bin/env python3.11
"""
æ¨èç³»ç»Ÿå®Œæ•´æµ‹è¯•å¥—ä»¶
æ•´åˆæ‰€æœ‰æ¨èé€»è¾‘ç›¸å…³çš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
"""

import unittest
import json
import requests
import time
from pathlib import Path
import sys
from unittest.mock import patch, MagicMock

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


class TestRecommendationCore(unittest.TestCase):
    """æµ‹è¯•æ¨èç³»ç»Ÿæ ¸å¿ƒé€»è¾‘"""
    
    def setUp(self):
        """æµ‹è¯•åˆå§‹åŒ–"""
        # æ ‡å‡†é…ç½®æ•°æ® (åŸºäºå®é™…çš„config.yaml)
        self.mock_config = {
            'content_classification': {
                'content_types': {
                    'lecture': {
                        'icon': 'ğŸ“',
                        'display_name': 'Academic Lecture',
                        'recommendations': {
                            'english': [
                                'distil-whisper_distil-large-v3',
                                'openai_whisper-medium'
                            ],
                            'multilingual': [
                                'openai_whisper-large-v3',
                                'openai_whisper-large-v3-v20240930'
                            ]
                        }
                    },
                    'meeting': {
                        'icon': 'ğŸ¢',
                        'display_name': 'Meeting Recording',
                        'recommendations': {
                            'english': [
                                'openai_whisper-large-v3',
                                'elevenlabs_api_speech'
                            ],
                            'multilingual': [
                                'openai_whisper-large-v3',
                                'elevenlabs_api_speech'
                            ]
                        }
                    },
                    'others': {
                        'icon': 'ğŸ“„',
                        'display_name': 'Others',
                        'recommendations': {
                            'english': [
                                'distil-whisper_distil-large-v3',
                                'openai_whisper-medium'
                            ],
                            'multilingual': [
                                'openai_whisper-large-v3',
                                'openai_whisper-medium'
                            ]
                        }
                    }
                }
            }
        }
        
        # æ ‡å‡†æ¨¡å‹æ•°æ®
        self.mock_models = [
            {
                'value': 'distil-whisper_distil-large-v3',
                'name': 'distil-whisper_distil-large-v3',
                'display_name': 'Distil Large V3',
                'english_support': True,
                'multilingual_support': False
            },
            {
                'value': 'openai_whisper-medium',
                'name': 'openai_whisper-medium',
                'display_name': 'OpenAI Medium',
                'english_support': True,
                'multilingual_support': True
            },
            {
                'value': 'openai_whisper-large-v3',
                'name': 'openai_whisper-large-v3',
                'display_name': 'OpenAI Large V3',
                'english_support': True,
                'multilingual_support': True
            },
            {
                'value': 'openai_whisper-large-v3-v20240930',
                'name': 'openai_whisper-large-v3-v20240930',
                'display_name': 'OpenAI Large V3 2024-09-30',
                'english_support': True,
                'multilingual_support': True
            },
            {
                'value': 'elevenlabs_api_speech',
                'name': 'ElevenLabs Speech Recognition',
                'display_name': 'ğŸ—£ï¸ ElevenLabs Speech Recognition',
                'english_support': True,
                'multilingual_support': True,
                'requires_api_key': True
            }
        ]

    def test_content_type_recommendation_mapping(self):
        """æµ‹è¯•å†…å®¹ç±»å‹æ¨èæ˜ å°„çš„æ­£ç¡®æ€§"""
        content_types = self.mock_config['content_classification']['content_types']
        
        # éªŒè¯lectureç±»å‹æ¨è
        lecture_recs = content_types['lecture']['recommendations']
        self.assertEqual(len(lecture_recs['english']), 2, "Lectureè‹±æ–‡æ¨èåº”è¯¥æ˜¯2ä¸ª")
        self.assertEqual(len(lecture_recs['multilingual']), 2, "Lectureå¤šè¯­è¨€æ¨èåº”è¯¥æ˜¯2ä¸ª")
        
        # éªŒè¯meetingç±»å‹æ¨è (ä¸lectureä¸åŒ)
        meeting_recs = content_types['meeting']['recommendations']
        self.assertNotEqual(
            set(lecture_recs['english']), 
            set(meeting_recs['english']),
            "Lectureå’ŒMeetingçš„è‹±æ–‡æ¨èåº”è¯¥ä¸åŒ"
        )
        
        # éªŒè¯meetingåŒ…å«ElevenLabs (ç”¨äºspeaker diarization)
        self.assertIn('elevenlabs_api_speech', meeting_recs['english'])
        self.assertNotIn('elevenlabs_api_speech', lecture_recs['english'])

    def test_exact_string_matching_logic(self):
        """æµ‹è¯•ç²¾ç¡®å­—ç¬¦ä¸²åŒ¹é…é€»è¾‘ (é˜²æ­¢substringåŒ¹é…é”™è¯¯)"""
        # åˆ›å»ºæ˜“æ··æ·†çš„æ¨¡å‹åç§°
        confusing_models = [
            'openai_whisper-large-v3',
            'openai_whisper-large-v3-v20240930'
        ]
        
        recommendations = ['openai_whisper-large-v3']  # åªæ¨èå…¶ä¸­ä¸€ä¸ª
        
        # æµ‹è¯•ç²¾ç¡®åŒ¹é…
        for model_name in confusing_models:
            is_recommended = any(rec == model_name for rec in recommendations)
            
            if model_name == 'openai_whisper-large-v3':
                self.assertTrue(is_recommended, "åº”è¯¥ç²¾ç¡®åŒ¹é…openai_whisper-large-v3")
            else:
                self.assertFalse(is_recommended, "ä¸åº”è¯¥åŒ¹é…åŒ…å«æ›´å¤šå­—ç¬¦çš„æ¨¡å‹å")

    def test_backend_recommendation_flag_setting(self):
        """æµ‹è¯•åç«¯æ¨èæ ‡å¿—è®¾ç½®é€»è¾‘"""
        
        def apply_content_type_recommendations(models, content_type, config):
            """æ¨¡æ‹Ÿåç«¯æ¨èæ ‡å¿—è®¾ç½®"""
            content_recommendations = config['content_classification']['content_types'][content_type]['recommendations']
            english_recs = content_recommendations.get('english', [])
            multilingual_recs = content_recommendations.get('multilingual', [])
            
            result_models = []
            for model in models:
                model_copy = model.copy()
                model_value = model_copy.get('value', '')
                model_name = model_copy.get('name', '')
                
                # ç²¾ç¡®åŒ¹é…è®¾ç½®æ¨èæ ‡å¿—
                model_copy['is_english_recommended'] = any(
                    rec_model == model_value or rec_model == model_name 
                    for rec_model in english_recs
                )
                model_copy['is_multilingual_recommended'] = any(
                    rec_model == model_value or rec_model == model_name
                    for rec_model in multilingual_recs
                )
                
                result_models.append(model_copy)
            
            return result_models
        
        # æµ‹è¯•lectureç±»å‹
        lecture_models = apply_content_type_recommendations(
            self.mock_models, 'lecture', self.mock_config
        )
        
        # éªŒè¯æ¨èæ ‡å¿—æ­£ç¡®æ€§
        for model in lecture_models:
            model_value = model['value']
            
            if model_value == 'distil-whisper_distil-large-v3':
                self.assertTrue(model['is_english_recommended'], "Distilåº”è¯¥è¢«æ¨èä¸ºè‹±æ–‡æ¨¡å‹")
                self.assertFalse(model['is_multilingual_recommended'], "Distilä¸åº”è¯¥è¢«æ¨èä¸ºå¤šè¯­è¨€æ¨¡å‹")
            
            elif model_value == 'openai_whisper-medium':
                self.assertTrue(model['is_english_recommended'], "Mediumåº”è¯¥è¢«æ¨èä¸ºè‹±æ–‡æ¨¡å‹")
                self.assertFalse(model['is_multilingual_recommended'], "Mediumä¸åº”è¯¥è¢«æ¨èä¸ºå¤šè¯­è¨€æ¨¡å‹ (åœ¨lectureé…ç½®ä¸­)")
            
            elif model_value == 'openai_whisper-large-v3':
                self.assertFalse(model['is_english_recommended'], "Large-v3ä¸åº”è¯¥è¢«æ¨èä¸ºè‹±æ–‡æ¨¡å‹ (åœ¨lectureé…ç½®ä¸­)")
                self.assertTrue(model['is_multilingual_recommended'], "Large-v3åº”è¯¥è¢«æ¨èä¸ºå¤šè¯­è¨€æ¨¡å‹")

    def test_frontend_filtering_and_display(self):
        """æµ‹è¯•å‰ç«¯è¿‡æ»¤å’Œæ˜¾ç¤ºé€»è¾‘"""
        
        # æ¨¡æ‹Ÿå‰ç«¯è¿‡æ»¤å‡½æ•°
        def filter_and_format_models(models, language_mode, content_type):
            filtered = []
            for model in models:
                # è¯­è¨€æ”¯æŒæ£€æŸ¥
                if language_mode == 'english':
                    language_supported = model.get('is_english_recommended') or model.get('english_support')
                    is_recommended = model.get('is_english_recommended', False)
                else:  # multilingual
                    language_supported = model.get('is_multilingual_recommended') or model.get('multilingual_support')
                    is_recommended = model.get('is_multilingual_recommended', False)
                
                if language_supported:
                    # æ ¼å¼åŒ–æ˜¾ç¤ºåç§°
                    display_name = model.get('display_name', model.get('name', ''))
                    if is_recommended:
                        display_name += ' â­'  # æ˜Ÿå·åœ¨æœ«å°¾
                    
                    filtered.append({
                        'value': model['value'],
                        'display_name': display_name,
                        'recommended': is_recommended
                    })
            
            return filtered
        
        # å‡†å¤‡å¸¦æ¨èæ ‡å¿—çš„æµ‹è¯•æ¨¡å‹
        test_models = []
        for model in self.mock_models:
            model_copy = model.copy()
            # æ¨¡æ‹Ÿlectureæ¨èæ ‡å¿—
            model_copy['is_english_recommended'] = model['value'] in [
                'distil-whisper_distil-large-v3', 'openai_whisper-medium'
            ]
            model_copy['is_multilingual_recommended'] = model['value'] in [
                'openai_whisper-large-v3', 'openai_whisper-large-v3-v20240930'
            ]
            test_models.append(model_copy)
        
        # æµ‹è¯•è‹±æ–‡æ¨¡å¼æ˜¾ç¤º
        english_display = filter_and_format_models(test_models, 'english', 'lecture')
        
        # éªŒè¯æ¨èæ¨¡å‹æœ‰æ˜Ÿå·
        recommended_english = [m for m in english_display if m['recommended']]
        self.assertEqual(len(recommended_english), 2, "åº”è¯¥æœ‰2ä¸ªè‹±æ–‡æ¨èæ¨¡å‹")
        
        for model in recommended_english:
            self.assertIn('â­', model['display_name'], f"æ¨èæ¨¡å‹{model['value']}åº”è¯¥æœ‰æ˜Ÿå·")
        
        # éªŒè¯æ˜Ÿå·åœ¨æœ«å°¾
        for model in english_display:
            if model['recommended']:
                self.assertTrue(model['display_name'].endswith(' â­'), "æ˜Ÿå·åº”è¯¥åœ¨åç§°æœ«å°¾")

    def test_model_sorting_priority(self):
        """æµ‹è¯•æ¨¡å‹æ’åºä¼˜å…ˆçº§"""
        
        def get_sort_priority(model):
            """æ¨¡æ‹Ÿæ’åºé€»è¾‘ï¼šæ¨èä¼˜å…ˆï¼Œç„¶åæŒ‰å¤æ‚åº¦"""
            is_recommended = model.get('recommended', False)
            config_info = model.get('config_info', {})
            
            # åŸºäºæ¨¡å‹å¤æ‚åº¦
            d_model = config_info.get('d_model', 0)
            encoder_layers = config_info.get('encoder_layers', 0)
            complexity = d_model * encoder_layers
            
            return (not is_recommended, -complexity)
        
        # å‡†å¤‡æµ‹è¯•æ¨¡å‹
        test_models = []
        for model in self.mock_models:
            model_copy = model.copy()
            model_copy['recommended'] = model['value'] in [
                'distil-whisper_distil-large-v3', 'openai_whisper-medium'
            ]
            # æ·»åŠ æ¨¡æ‹Ÿé…ç½®ä¿¡æ¯
            if 'distil' in model['value']:
                model_copy['config_info'] = {'d_model': 1280, 'encoder_layers': 32}
            elif 'medium' in model['value']:
                model_copy['config_info'] = {'d_model': 1024, 'encoder_layers': 24}
            else:
                model_copy['config_info'] = {'d_model': 1280, 'encoder_layers': 32}
            
            test_models.append(model_copy)
        
        # æ’åº
        sorted_models = sorted(test_models, key=get_sort_priority)
        
        # éªŒè¯æ¨èæ¨¡å‹åœ¨å‰é¢
        first_two = sorted_models[:2]
        self.assertTrue(all(m['recommended'] for m in first_two), 
                       "å‰ä¸¤ä¸ªæ¨¡å‹åº”è¯¥éƒ½æ˜¯æ¨èçš„")


class TestRecommendationAPIIntegration(unittest.TestCase):
    """æµ‹è¯•æ¨èç³»ç»Ÿä¸Web APIçš„é›†æˆ"""
    
    @classmethod
    def setUpClass(cls):
        """æµ‹è¯•ç±»åˆå§‹åŒ–"""
        cls.api_base_url = 'http://localhost:8080'
        cls.timeout = 5
        
        # æ£€æŸ¥APIå¯ç”¨æ€§
        try:
            response = requests.get(f'{cls.api_base_url}/api/models/smart-config', timeout=3)
            cls.api_available = response.status_code == 200
        except:
            cls.api_available = False
    
    def setUp(self):
        """æ¯ä¸ªæµ‹è¯•å‰çš„åˆå§‹åŒ–"""
        if not self.api_available:
            self.skipTest("Web APIæœåŠ¡ä¸å¯ç”¨")

    def test_api_response_structure_completeness(self):
        """æµ‹è¯•APIå“åº”ç»“æ„çš„å®Œæ•´æ€§"""
        response = requests.get(f'{self.api_base_url}/api/models/smart-config', timeout=self.timeout)
        
        self.assertEqual(response.status_code, 200)
        data = response.json()
        
        # éªŒè¯å¿…éœ€çš„å†…å®¹ç±»å‹
        required_types = ['lecture', 'meeting', 'others', 'all']
        for content_type in required_types:
            self.assertIn(content_type, data, f"ç¼ºå°‘{content_type}ç±»å‹")
            self.assertIsInstance(data[content_type], list, f"{content_type}åº”è¯¥æ˜¯æ•°ç»„")

    def test_lecture_recommendations_count_accuracy(self):
        """æµ‹è¯•lectureæ¨èæ•°é‡çš„å‡†ç¡®æ€§"""
        response = requests.get(f'{self.api_base_url}/api/models/smart-config', timeout=self.timeout)
        data = response.json()
        
        lecture_models = data['lecture']
        
        # ç»Ÿè®¡è‹±æ–‡å’Œå¤šè¯­è¨€æ¨è
        english_recommended = [m for m in lecture_models if m.get('is_english_recommended') == True]
        multilingual_recommended = [m for m in lecture_models if m.get('is_multilingual_recommended') == True]
        
        # æ ¹æ®é…ç½®ï¼Œè‹±æ–‡åº”è¯¥æœ‰2ä¸ªæ¨èï¼Œå¤šè¯­è¨€åº”è¯¥æœ‰1ä¸ªæ¨è (åŸºäºconfig.yaml)
        self.assertEqual(len(english_recommended), 2, 
                        f"Lectureè‹±æ–‡æ¨èåº”è¯¥æ˜¯2ä¸ªï¼Œå®é™…{len(english_recommended)}ä¸ª")
        self.assertEqual(len(multilingual_recommended), 1, 
                        f"Lectureå¤šè¯­è¨€æ¨èåº”è¯¥æ˜¯1ä¸ªï¼Œå®é™…{len(multilingual_recommended)}ä¸ª")
        
        # éªŒè¯å…·ä½“æ¨èçš„æ¨¡å‹
        english_values = {m['value'] for m in english_recommended}
        expected_english = {'distil-whisper_distil-large-v3', 'openai_whisper-medium'}
        self.assertEqual(english_values, expected_english, "Lectureè‹±æ–‡æ¨èæ¨¡å‹ä¸æ­£ç¡®")

    def test_meeting_vs_lecture_recommendation_difference(self):
        """æµ‹è¯•meetingä¸lectureæ¨èçš„å·®å¼‚æ€§"""
        response = requests.get(f'{self.api_base_url}/api/models/smart-config', timeout=self.timeout)
        data = response.json()
        
        lecture_models = data['lecture']
        meeting_models = data['meeting']
        
        # è·å–è‹±æ–‡æ¨è
        lecture_english = {m['value'] for m in lecture_models if m.get('is_english_recommended')}
        meeting_english = {m['value'] for m in meeting_models if m.get('is_english_recommended')}
        
        # åº”è¯¥ä¸åŒ
        self.assertNotEqual(lecture_english, meeting_english, 
                          "Lectureå’ŒMeetingçš„è‹±æ–‡æ¨èåº”è¯¥ä¸åŒ")
        
        # Meetingåº”è¯¥æ¨èElevenLabsï¼ŒLectureä¸åº”è¯¥
        self.assertIn('elevenlabs_api_speech', meeting_english, "Meetingåº”è¯¥æ¨èElevenLabs")
        self.assertNotIn('elevenlabs_api_speech', lecture_english, "Lectureä¸åº”è¯¥æ¨èElevenLabs")
        
        # Meetingåº”è¯¥æ¨èé«˜ç²¾åº¦æ¨¡å‹
        self.assertIn('openai_whisper-large-v3', meeting_english, "Meetingåº”è¯¥æ¨èLarge-v3")

    def test_configuration_api_consistency(self):
        """æµ‹è¯•é…ç½®æ–‡ä»¶ä¸APIçš„ä¸€è‡´æ€§"""
        from src.utils.config import ConfigManager
        
        # è·å–å®é™…é…ç½®
        config_manager = ConfigManager()
        config = config_manager.get_full_config()
        
        # è·å–APIæ•°æ®
        response = requests.get(f'{self.api_base_url}/api/models/smart-config', timeout=self.timeout)
        api_data = response.json()
        
        # éªŒè¯é…ç½®ä¸APIçš„ä¸€è‡´æ€§
        content_types = config['content_classification']['content_types']
        
        for content_type, type_config in content_types.items():
            if content_type in api_data:
                api_models = api_data[content_type]
                config_recommendations = type_config.get('recommendations', {})
                
                if isinstance(config_recommendations, dict):
                    # éªŒè¯è‹±æ–‡æ¨èä¸€è‡´æ€§
                    config_english = set(config_recommendations.get('english', []))
                    api_english = {m['value'] for m in api_models if m.get('is_english_recommended')}
                    
                    self.assertEqual(config_english, api_english,
                                   f"{content_type}é…ç½®ä¸APIè‹±æ–‡æ¨èä¸ä¸€è‡´")

    def test_api_performance_requirements(self):
        """æµ‹è¯•APIæ€§èƒ½è¦æ±‚"""
        start_time = time.time()
        response = requests.get(f'{self.api_base_url}/api/models/smart-config', timeout=self.timeout)
        response_time = time.time() - start_time
        
        # æ€§èƒ½è¦æ±‚
        self.assertLess(response_time, 2.0, f"APIå“åº”æ—¶é—´è¿‡é•¿: {response_time:.2f}s")
        self.assertEqual(response.status_code, 200, "APIåº”è¯¥è¿”å›æˆåŠŸçŠ¶æ€")
        
        # å“åº”å¤§å°åˆç†æ€§
        response_size = len(response.content)
        self.assertGreater(response_size, 1000, "å“åº”å†…å®¹å¤ªå°‘")
        self.assertLess(response_size, 100000, "å“åº”å†…å®¹å¤ªå¤§")


class TestRecommendationEdgeCases(unittest.TestCase):
    """æµ‹è¯•æ¨èç³»ç»Ÿçš„è¾¹ç•Œæƒ…å†µå’Œé”™è¯¯å¤„ç†"""
    
    def test_empty_recommendation_lists(self):
        """æµ‹è¯•ç©ºæ¨èåˆ—è¡¨çš„å¤„ç†"""
        empty_config = {
            'recommendations': {
                'english': [],
                'multilingual': []
            }
        }
        
        test_models = [
            {'value': 'model1', 'name': 'Model 1'},
            {'value': 'model2', 'name': 'Model 2'}
        ]
        
        # æ¨¡æ‹Ÿç©ºæ¨èå¤„ç†
        def process_empty_recommendations(models, config):
            english_recs = config['recommendations']['english']
            multilingual_recs = config['recommendations']['multilingual']
            
            for model in models:
                model['is_english_recommended'] = model['value'] in english_recs
                model['is_multilingual_recommended'] = model['value'] in multilingual_recs
            
            return models
        
        result = process_empty_recommendations(test_models, empty_config)
        
        # æ‰€æœ‰æ¨¡å‹éƒ½ä¸åº”è¯¥è¢«æ¨è
        for model in result:
            self.assertFalse(model['is_english_recommended'])
            self.assertFalse(model['is_multilingual_recommended'])

    def test_malformed_model_data_handling(self):
        """æµ‹è¯•æ ¼å¼é”™è¯¯çš„æ¨¡å‹æ•°æ®å¤„ç†"""
        malformed_models = [
            {},  # å®Œå…¨ç©ºçš„æ¨¡å‹
            {'value': ''},  # ç©ºvalue
            {'name': ''},   # ç©ºname
            {'value': None, 'name': None},  # Noneå€¼
            {'value': 'valid_model'},  # ç¼ºå°‘name
            {'name': 'valid_model'},   # ç¼ºå°‘value
        ]
        
        recommendations = ['valid_model']
        
        # å¥å£®çš„åŒ¹é…é€»è¾‘
        def robust_matching(model, recommendations):
            try:
                model_value = model.get('value') or ''
                model_name = model.get('name') or ''
                
                return any(
                    rec and (rec == model_value or rec == model_name)
                    for rec in recommendations
                )
            except Exception:
                return False
        
        # æµ‹è¯•æ‰€æœ‰æ ¼å¼é”™è¯¯çš„æ¨¡å‹
        for model in malformed_models:
            try:
                result = robust_matching(model, recommendations)
                self.assertIsInstance(result, bool, f"åº”è¯¥è¿”å›å¸ƒå°”å€¼ï¼Œæ¨¡å‹: {model}")
            except Exception as e:
                self.fail(f"å¤„ç†æ ¼å¼é”™è¯¯æ¨¡å‹æ—¶æŠ›å‡ºå¼‚å¸¸: {e}, æ¨¡å‹: {model}")

    def test_duplicate_model_handling(self):
        """æµ‹è¯•é‡å¤æ¨¡å‹çš„å¤„ç†"""
        duplicate_models = [
            {'value': 'same_model', 'name': 'Model A'},
            {'value': 'same_model', 'name': 'Model B'},  # ç›¸åŒvalue
            {'value': 'model_c', 'name': 'same_model'},  # nameä¸ç¬¬ä¸€ä¸ªvalueç›¸åŒ
        ]
        
        recommendations = ['same_model']
        
        # åº”è¯¥æ­£ç¡®åŒ¹é…æ‰€æœ‰ç›¸å…³æ¨¡å‹
        matches = []
        for model in duplicate_models:
            model_value = model.get('value', '')
            model_name = model.get('name', '')
            
            if any(rec == model_value or rec == model_name for rec in recommendations):
                matches.append(model)
        
        # åº”è¯¥åŒ¹é…æ‰€æœ‰åŒ…å«'same_model'çš„æ¨¡å‹
        self.assertEqual(len(matches), 3, "åº”è¯¥åŒ¹é…æ‰€æœ‰ç›¸å…³æ¨¡å‹")

    def test_special_characters_in_model_names(self):
        """æµ‹è¯•æ¨¡å‹åç§°ä¸­çš„ç‰¹æ®Šå­—ç¬¦å¤„ç†"""
        special_models = [
            {'value': 'model-with-dashes', 'name': 'Model With Dashes'},
            {'value': 'model_with_underscores', 'name': 'Model With Underscores'},
            {'value': 'model.with.dots', 'name': 'Model With Dots'},
            {'value': 'model@with#special!chars', 'name': 'Model With Special Chars'},
        ]
        
        recommendations = ['model-with-dashes', 'model_with_underscores']
        
        # ç²¾ç¡®åŒ¹é…åº”è¯¥æ­£ç¡®å¤„ç†ç‰¹æ®Šå­—ç¬¦
        for model in special_models:
            model_value = model['value']
            is_recommended = model_value in recommendations
            
            if model_value in ['model-with-dashes', 'model_with_underscores']:
                self.assertTrue(is_recommended, f"{model_value}åº”è¯¥è¢«æ¨è")
            else:
                self.assertFalse(is_recommended, f"{model_value}ä¸åº”è¯¥è¢«æ¨è")


class TestRecommendationSystemRegression(unittest.TestCase):
    """å›å½’æµ‹è¯•ï¼šç¡®ä¿ä¹‹å‰ä¿®å¤çš„é—®é¢˜ä¸ä¼šå†æ¬¡å‡ºç°"""
    
    def test_substring_matching_regression(self):
        """å›å½’æµ‹è¯•ï¼šç¡®ä¿ä¸ä¼šå‘ç”ŸsubstringåŒ¹é…é”™è¯¯"""
        # è¿™æ˜¯ä¹‹å‰å‡ºç°è¿‡çš„é—®é¢˜ï¼šopenai_whisper-large-v3ä¼šåŒ¹é…openai_whisper-large-v3-v20240930
        confusing_pairs = [
            ('openai_whisper-large-v3', 'openai_whisper-large-v3-v20240930'),
            ('model-v1', 'model-v1-beta'),
            ('whisper-base', 'whisper-base-en'),
        ]
        
        for base_name, extended_name in confusing_pairs:
            recommendations = [base_name]  # åªæ¨èåŸºç¡€åç§°
            
            # æµ‹è¯•ç²¾ç¡®åŒ¹é…
            base_match = any(rec == base_name for rec in recommendations)
            extended_match = any(rec == extended_name for rec in recommendations)
            
            self.assertTrue(base_match, f"{base_name}åº”è¯¥è¢«åŒ¹é…")
            self.assertFalse(extended_match, f"{extended_name}ä¸åº”è¯¥è¢«åŒ¹é… (substringé—®é¢˜)")

    def test_star_position_regression(self):
        """å›å½’æµ‹è¯•ï¼šç¡®ä¿æ˜Ÿå·åœ¨æ¨¡å‹åç§°æœ«å°¾"""
        test_models = [
            {'display_name': 'Model A', 'recommended': True},
            {'display_name': 'Model B', 'recommended': False},
        ]
        
        # æ¨¡æ‹Ÿå‰ç«¯æ˜¾ç¤ºé€»è¾‘
        for model in test_models:
            display_name = model['display_name']
            if model['recommended']:
                display_name += ' â­'  # æ˜Ÿå·åº”è¯¥åœ¨æœ«å°¾
            
            if model['recommended']:
                self.assertTrue(display_name.endswith(' â­'), 
                               f"æ¨èæ¨¡å‹çš„æ˜Ÿå·åº”è¯¥åœ¨æœ«å°¾: {display_name}")
                self.assertFalse(display_name.startswith('â­'), 
                                f"æ˜Ÿå·ä¸åº”è¯¥åœ¨å¼€å¤´: {display_name}")
            else:
                self.assertNotIn('â­', display_name, f"éæ¨èæ¨¡å‹ä¸åº”è¯¥æœ‰æ˜Ÿå·: {display_name}")

    def test_content_type_isolation_regression(self):
        """å›å½’æµ‹è¯•ï¼šç¡®ä¿ä¸åŒå†…å®¹ç±»å‹çš„æ¨èç›¸äº’éš”ç¦»"""
        # è¿™æ˜¯ä¹‹å‰å‡ºç°çš„é—®é¢˜ï¼šæ‰€æœ‰å†…å®¹ç±»å‹éƒ½æ˜¾ç¤ºç›¸åŒçš„æ¨è
        
        config = {
            'lecture': {
                'recommendations': {
                    'english': ['model_a', 'model_b']
                }
            },
            'meeting': {
                'recommendations': {
                    'english': ['model_c', 'model_d']
                }
            }
        }
        
        models = [
            {'value': 'model_a'}, {'value': 'model_b'},
            {'value': 'model_c'}, {'value': 'model_d'}
        ]
        
        # æ¨¡æ‹Ÿå†…å®¹ç±»å‹ç‰¹å®šçš„æ¨èè®¾ç½®
        def set_content_type_recommendations(models, content_type, config):
            content_recs = config[content_type]['recommendations']['english']
            
            for model in models:
                model[f'{content_type}_english_recommended'] = model['value'] in content_recs
            
            return models
        
        # å¤„ç†ä¸åŒå†…å®¹ç±»å‹
        processed_models = models.copy()
        for content_type in ['lecture', 'meeting']:
            processed_models = set_content_type_recommendations(
                processed_models, content_type, config
            )
        
        # éªŒè¯éš”ç¦»æ€§
        for model in processed_models:
            lecture_rec = model.get('lecture_english_recommended', False)
            meeting_rec = model.get('meeting_english_recommended', False)
            
            # åº”è¯¥æœ‰ä¸åŒçš„æ¨è
            if model['value'] in ['model_a', 'model_b']:
                self.assertTrue(lecture_rec, f"{model['value']}åº”è¯¥è¢«lectureæ¨è")
                self.assertFalse(meeting_rec, f"{model['value']}ä¸åº”è¯¥è¢«meetingæ¨è")
            else:
                self.assertFalse(lecture_rec, f"{model['value']}ä¸åº”è¯¥è¢«lectureæ¨è")
                self.assertTrue(meeting_rec, f"{model['value']}åº”è¯¥è¢«meetingæ¨è")


def run_all_recommendation_tests():
    """è¿è¡Œæ‰€æœ‰æ¨èç³»ç»Ÿæµ‹è¯•"""
    
    print("ğŸ§ª å¼€å§‹æ¨èç³»ç»Ÿå®Œæ•´æµ‹è¯•å¥—ä»¶...")
    print("=" * 80)
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()
    
    # æ·»åŠ æ‰€æœ‰æµ‹è¯•ç±»
    test_classes = [
        TestRecommendationCore,
        TestRecommendationAPIIntegration,
        TestRecommendationEdgeCases,
        TestRecommendationSystemRegression
    ]
    
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    # æµ‹è¯•ç»“æœæ€»ç»“
    print("\n" + "=" * 80)
    print(f"ğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"   æ€»æµ‹è¯•æ•°: {result.testsRun}")
    print(f"   æˆåŠŸ: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"   å¤±è´¥: {len(result.failures)}")
    print(f"   é”™è¯¯: {len(result.errors)}")
    print(f"   è·³è¿‡: {len(result.skipped) if hasattr(result, 'skipped') else 0}")
    
    if result.failures:
        print("\nâŒ å¤±è´¥çš„æµ‹è¯•:")
        for test, traceback in result.failures:
            print(f"   - {test}")
    
    if result.errors:
        print("\nâš ï¸  é”™è¯¯çš„æµ‹è¯•:")
        for test, traceback in result.errors:
            print(f"   - {test}")
    
    success_rate = ((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100) if result.testsRun > 0 else 0
    print(f"\nâœ… æˆåŠŸç‡: {success_rate:.1f}%")
    
    return result.wasSuccessful()


if __name__ == '__main__':
    # å¯ä»¥å•ç‹¬è¿è¡ŒæŸä¸ªæµ‹è¯•ç±»
    if len(sys.argv) > 1:
        unittest.main()
    else:
        # è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
        success = run_all_recommendation_tests()
        sys.exit(0 if success else 1)